on the train project to attempt to copy amazons statistically improbable phrase calculations
data from project gutenberg, runs using hadoop streaming with ruby map/reduce functions

see project page at http://matpalm.com/sip

bash> start-hadoop-here
bash> rake prepare_files input=input.eg # upload 8 file example
bash> rake upload_input

experiment take 2: term frequencies
bash> rake term_frequency_calculate_sips 
bash> rake cat dir=least_frequent_trigrams

experiment take 3: markov chains
*work in progress*

bash> # bask in glow of diysips
