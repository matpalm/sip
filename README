on the train project to attempt to copy amazons statistically improbable phrase calculations
data from project gutenberg, runs using hadoop streaming with ruby map/reduce functions

bash> start-hadoop-here
bash> rake insert_filename_at_start_and_remove_blanks input=input.eg # upload 8 file example
bash> rake upload_input

experiment take 2: term frequencies
bash> rake term_frequency:calc_sips 
bash> rake cat DIR=least_frequent_trigram

experiment take 3: markov chains
*work in progress*

bash> # bask in glow of diysips
